---
title: Projet 5
emoji: ğŸƒ
colorFrom: blue
colorTo: gray
sdk: docker
pinned: false
---

Check out the configuration reference at https://huggingface.co/docs/hub/spaces-config-reference

# API de PrÃ©diction de Turnover - SystÃ¨me de Classification ML

API RESTful dÃ©veloppÃ©e avec FastAPI pour prÃ©dire le risque de dÃ©part des employÃ©s en utilisant un modÃ¨le XGBoost. L'application intÃ¨gre une base de donnÃ©es PostgreSQL pour le stockage et le logging des prÃ©dictions, et est containerisÃ©e avec Docker pour faciliter le dÃ©ploiement.

## Table des matiÃ¨res

- [Architecture du Projet](#architecture-du-projet)
- [PrÃ©requis](#prÃ©requis)
- [Installation](#installation)
- [Configuration](#configuration)
- [Base de DonnÃ©es](#base-de-donnÃ©es)
- [Utilisation de l'API](#utilisation-de-lapi)
- [Tests](#tests)
- [DÃ©ploiement](#dÃ©ploiement)
- [SÃ©curitÃ©](#sÃ©curitÃ©)
- [Gestion des DonnÃ©es](#gestion-des-donnÃ©es)

---

## Architecture du Projet

### Structure des fichiers

```
projet_5_github/
â”œâ”€â”€ app/                          # Code source de l'application
â”‚   â”œâ”€â”€ main.py                   # Point d'entrÃ©e FastAPI avec lifespan
â”‚   â”œâ”€â”€ api.py                    # Configuration de l'API
â”‚   â”œâ”€â”€ routes.py                 # Endpoints de prÃ©diction
â”‚   â”œâ”€â”€ models.py                 # ModÃ¨les SQLAlchemy
â”‚   â”œâ”€â”€ schemas.py                # SchÃ©mas Pydantic (validation)
â”‚   â”œâ”€â”€ database.py               # Configuration DB avec retry logic
â”‚   â”œâ”€â”€ seed.py                   # Script d'initialisation des donnÃ©es
â”‚   â””â”€â”€ migrate.py                # Gestion des migrations
â”œâ”€â”€ database/
â”‚   â””â”€â”€ schema.sql                # SchÃ©ma PostgreSQL avec relations
â”œâ”€â”€ models/
â”‚   â””â”€â”€ model                     # ModÃ¨le XGBoost entraÃ®nÃ©
â”œâ”€â”€ test/
â”‚   â”œâ”€â”€ conftest.py               # Configuration pytest
â”‚   â””â”€â”€ test_api.py               # Tests unitaires et fonctionnels
â”œâ”€â”€ .github/workflows/
â”‚   â””â”€â”€ ci.yml                    # Pipeline CI/CD automatisÃ©
â”œâ”€â”€ docker-compose.yml            # Orchestration des services
â”œâ”€â”€ Dockerfile                    # Image de l'application
â”œâ”€â”€ pyproject.toml                # DÃ©pendances et configuration Python
â””â”€â”€ README.md                     # Documentation

```

### Stack Technique

- **API Framework** : FastAPI 0.128+ (async, haute performance)
- **ML Framework** : XGBoost 3.0.0 + scikit-learn 1.7.2
- **Base de donnÃ©es** : PostgreSQL 16-alpine
- **ORM** : SQLAlchemy 2.0+ avec modÃ¨les dÃ©claratifs
- **Validation** : Pydantic 2.12+ pour la sÃ©curitÃ© des donnÃ©es
- **Tests** : Pytest 9.0+ avec couverture de code
- **Containerisation** : Docker + Docker Compose
- **Gestionnaire de paquets** : UV (build moderne et rapide)
- **CI/CD** : GitHub Actions

---

## PrÃ©requis

### SystÃ¨me requis

- **Docker** : version 20.10+
- **Docker Compose** : version 2.0+
- **Python** : 3.12+ (pour dÃ©veloppement local)
- **UV** : gestionnaire de paquets (installation automatique via Docker)

### Ports utilisÃ©s

- `5432` : PostgreSQL
- `8000` : API FastAPI

---

## Installation

### Option 1 : Installation avec Docker (RecommandÃ©e)

Cette mÃ©thode garantit un environnement isolÃ© et reproductible.

```bash
# 1. Cloner le dÃ©pÃ´t
git clone https://github.com/votre-username/projet_5_github.git
cd projet_5_github

# 2. VÃ©rifier que PostgreSQL local n'utilise pas le port 5432
sudo systemctl stop postgresql  # Si installÃ© localement
sudo systemctl disable postgresql

# 3. Lancer tous les services (DB + API + Seed)
docker compose up -d

# 4. VÃ©rifier que les services sont actifs
docker compose ps

# 5. Consulter les logs
docker compose logs -f app
```

**L'API sera accessible sur** : http://localhost:7860

### Option 2 : Installation locale (DÃ©veloppement)

```bash
# 1. Installer UV
curl -LsSf https://astral.sh/uv/install.sh | sh

# 2. CrÃ©er un environnement virtuel et installer les dÃ©pendances
uv sync

# 3. DÃ©marrer PostgreSQL (avec Docker ou localement)
docker run -d \
  --name postgres_dev \
  -e POSTGRES_USER=postgres \
  -e POSTGRES_PASSWORD=mysecretpassword \
  -e POSTGRES_DB=employee_db \
  -p 5432:5432 \
  postgres:16-alpine

# 4. Initialiser la base de donnÃ©es
uv run python -c "from app.database import init_db; init_db()"

# 5. Charger les donnÃ©es
uv run python app/seed.py --csv-file data_merge.csv --update \
  --database-url postgresql://postgres:mysecretpassword@localhost:5432/employee_db

# 6. Lancer l'API
uv run uvicorn app.main:app --host 0.0.0.0 --port 7860 --reload
```

---

## Configuration

### Gestion des environnements

Le projet supporte plusieurs environnements via des variables d'environnement :

#### DÃ©veloppement (local)

```env
# .env.development
DATABASE_URL=postgresql://postgres:mysecretpassword@localhost:5432/employee_db
ENVIRONMENT=development
LOG_LEVEL=DEBUG
```

#### Test (CI/CD)

Les tests utilisent automatiquement un service PostgreSQL isolÃ© dans GitHub Actions :

```yaml
services:
  db:
    image: postgres:16-alpine
    env:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: mysecretpassword
      POSTGRES_DB: employee_db_test
```

#### Production

Variables gÃ©rÃ©es via secrets Docker/Kubernetes :

```env
DATABASE_URL=postgresql://${DB_USER}:${DB_PASSWORD}@${DB_HOST}:5432/${DB_NAME}
ENVIRONMENT=production
LOG_LEVEL=INFO
```

### Gestion des secrets

**SÃ©curitÃ© des credentials :**

1. **Jamais** de secrets dans le code source
2. Utilisation de variables d'environnement
3. Secrets stockÃ©s dans GitHub Actions Secrets
4. Hachage bcrypt pour les mots de passe utilisateurs
5. Connexions DB avec certificats SSL en production

**Secrets requis pour le CI/CD :**

```bash
# Dans GitHub Settings > Secrets and variables > Actions
HF_TOKEN=hf_xxxxxxxxxxxxx
HF_USERNAME=votre-username
HF_SPACE_NAME=nom-du-space
DB_PASSWORD=mot-de-passe-sÃ©curisÃ©
```

---

## Base de DonnÃ©es

### Architecture des donnÃ©es

Le systÃ¨me utilise PostgreSQL avec deux tables principales :

#### Table `employees`

Stocke les caractÃ©ristiques complÃ¨tes des employÃ©s :

```sql
CREATE TABLE employees (
    id_employee SERIAL PRIMARY KEY,
    age INT NOT NULL CHECK (age >= 18 AND age <= 100),
    genre VARCHAR(1) NOT NULL CHECK (genre IN ('M', 'F')),
    statut_marital VARCHAR(50),
    ayant_enfants BOOLEAN,
    revenu_mensuel FLOAT CHECK (revenu_mensuel > 0),
    departement VARCHAR(100),
    poste VARCHAR(100),
    -- ... 25+ autres champs
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    CONSTRAINT valid_genre CHECK (genre IN ('M', 'F'))
);
```

**Contraintes d'intÃ©gritÃ©** :
- ClÃ© primaire auto-incrÃ©mentÃ©e
- Validation des types (Ã¢ge, genre, revenu)
- Timestamps automatiques
- Index sur `id_employee` pour performances

#### Table `predictions`

Enregistre chaque interaction utilisateur avec le modÃ¨le ML :

```sql
CREATE TABLE predictions (
    id SERIAL PRIMARY KEY,
    id_employee INT NOT NULL,
    prediction INT NOT NULL CHECK (prediction IN (0, 1)),
    confidence FLOAT CHECK (confidence BETWEEN 0 AND 1),
    risk_level VARCHAR(20),
    probability_reste FLOAT,
    probability_quitte FLOAT,
    model_version VARCHAR(50),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (id_employee) REFERENCES employees(id_employee) 
        ON DELETE CASCADE
);
```

**Relations** :
- ClÃ© Ã©trangÃ¨re vers `employees` avec CASCADE
- Permet de tracker l'historique complet des prÃ©dictions
- IndexÃ© sur `id_employee` et `created_at` pour les requÃªtes analytiques

### SchÃ©ma relationnel

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   employees     â”‚ 1     âˆ â”‚   predictions    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â—„â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ id_employee (PK)â”‚         â”‚ id (PK)          â”‚
â”‚ age             â”‚         â”‚ id_employee (FK) â”‚
â”‚ genre           â”‚         â”‚ prediction       â”‚
â”‚ departement     â”‚         â”‚ confidence       â”‚
â”‚ ...             â”‚         â”‚ created_at       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Initialisation de la base

Le script `database/schema.sql` est exÃ©cutÃ© automatiquement au dÃ©marrage du conteneur PostgreSQL :

```yaml
# docker-compose.yml
db:
  image: postgres:16-alpine
  volumes:
    - ./database/schema.sql:/docker-entrypoint-initdb.d/init.sql
```

### Injection des donnÃ©es

Le script `app/seed.py` permet d'importer/mettre Ã  jour les donnÃ©es depuis CSV :

```bash
# Chargement initial
uv run ./app/seed.py \
  --csv-file data_merge.csv \
  --database-url postgresql://postgres:mysecretpassword@db:5432/employee_db

# Mise Ã  jour (upsert)
uv run ./app/seed.py \
  --csv-file data_merge.csv \
  --update \
  --batch-size 1000 \
  --database-url postgresql://postgres:mysecretpassword@db:5432/employee_db
```

**FonctionnalitÃ©s du seeder** :
- Validation Pydantic des donnÃ©es avant insertion
- Gestion des erreurs avec retry logic (30 tentatives)
- Insertion par batch pour performance
- Mode upsert (update si existe)
- Logging dÃ©taillÃ© des opÃ©rations

### Gestion du volume et performances

**ScalabilitÃ©** :
- Index sur colonnes frÃ©quemment requÃªtÃ©es
- Partitionnement par date possible pour `predictions`
- Connection pooling via SQLAlchemy (pool_pre_ping=True)
- Batch processing (1000 enregistrements/lot)

**VolumÃ©trie actuelle** :
- ~1470 employÃ©s
---

## Utilisation de l'API

### Documentation interactive

FastAPI gÃ©nÃ¨re automatiquement une documentation OpenAPI :

- **Swagger UI** : http://localhost:7860/docs
- **ReDoc** : http://localhost:7860/redoc

### Endpoints disponibles

#### 1. Racine de l'API

```bash
GET /

# RÃ©ponse
{
  "message": "Bienvenue sur l'API de classification"
}
```

#### 2. PrÃ©diction pour un nouvel employÃ©

```bash
POST /predict_employee

# Headers
Content-Type: application/json

# Body (exemple complet)
{
  "id_employee": 123,
  "age": 35,
  "genre": "M",
  "statut_marital": "Married",
  "ayant_enfants": true,
  "revenu_mensuel": 5000.0,
  "departement": "Sales",
  "poste": "Manager",
  "niveau_hierarchique_poste": 3,
  "heure_supplementaires": "Yes",
  "distance_domicile_travail": 10.5,
  "annees_dans_l_entreprise": 5.0,
  "satisfaction_employee_environnement": 4,
  "note_evaluation_actuelle": 3.5,
  ...
}

# RÃ©ponse
{
  "id_employee": 123,
  "prediction": 1,
  "confidence": 0.87
}
```

**Codes de retour** :
- `200` : PrÃ©diction rÃ©ussie
- `400` : DonnÃ©es invalides (validation Pydantic)
- `500` : Erreur serveur

#### 3. PrÃ©diction par ID employÃ© existant

```bash
GET /predict_employee/{id_employee}

# Exemple
curl http://localhost:7860/predict_employee/42

# RÃ©ponse
{
  "id_employee": 42,
  "prediction": 0,
  "confidence": 0.92
}
```

**Codes de retour** :
- `200` : EmployÃ© trouvÃ© et prÃ©dit
- `404` : EmployÃ© non trouvÃ©
- `500` : Erreur serveur

### Validation des donnÃ©es

Toutes les entrÃ©es sont validÃ©es par Pydantic avant traitement :

```python
# app/schemas.py
class EmployeeInput(BaseModel):
    id_employee: int
    age: int = Field(ge=18, le=100)
    genre: str = Field(pattern=r'^[MF]$')
    revenu_mensuel: float = Field(gt=0)
    # ... autres validations
```

**Erreurs de validation** :

```json
{
  "detail": [
    {
      "loc": ["body", "age"],
      "msg": "ensure this value is greater than or equal to 18",
      "type": "value_error.number.not_ge"
    }
  ]
}
```

### Logging des interactions

Chaque prÃ©diction est automatiquement enregistrÃ©e dans la table `predictions` :

```python
# app/routes.py
def save_prediction(db, id_employee, prediction, probabilities):
    new_prediction = Prediction(
        id_employee=id_employee,
        prediction=int(prediction),
        confidence=float(np.max(probabilities)),
        probability_reste=float(probabilities[0]),
        probability_quitte=float(probabilities[1]),
        risk_level="Haut" if prediction == 1 else "Normal",
        model_version="1.0.0"
    )
    db.add(new_prediction)
    db.commit()
```

---

## Tests

### Framework de tests

Le projet utilise **Pytest** avec couverture de code automatique :

```toml
# pyproject.toml
[tool.pytest.ini_options]
addopts = "--cov=app --cov-report=term-missing --cov-report=html"
testpaths = ["test"]
python_files = "test_*.py"
```

### ExÃ©cution des tests

```bash
# Tous les tests avec couverture
uv run pytest

# Tests spÃ©cifiques
uv run pytest test/test_api.py::test_predict_employee

# Mode verbose
uv run pytest -v -s

# Rapport de couverture HTML
uv run pytest --cov-report=html
# Ouvrir htmlcov/index.html
```

### Cas de tests couverts

#### Tests unitaires

```python
# test/test_api.py
def test_predict_employee_valid_data(client):
    """Test prÃ©diction avec donnÃ©es valides"""
    response = client.post("/predict_employee", json=valid_employee)
    assert response.status_code == 200
    assert "prediction" in response.json()

def test_predict_employee_invalid_age(client):
    """Test validation Ã¢ge < 18"""
    response = client.post("/predict_employee", json={
        "age": 15,  # Invalide
        ...
    })
    assert response.status_code == 422
```

#### Tests fonctionnels

```python
def test_prediction_persistence(client, db):
    """VÃ©rifie que la prÃ©diction est enregistrÃ©e en base"""
    response = client.post("/predict_employee", json=valid_employee)
    
    # VÃ©rifier en base
    prediction = db.query(Prediction).filter_by(
        id_employee=valid_employee["id_employee"]
    ).first()
    assert prediction is not None
    assert prediction.confidence > 0
```

#### Tests d'intÃ©gritÃ©

```python
def test_database_constraints(db):
    """Test des contraintes DB (FK, CHECK, etc.)"""
    with pytest.raises(IntegrityError):
        # Tentative d'insertion avec FK invalide
        db.add(Prediction(id_employee=99999, ...))
        db.commit()
```

### Couverture de code

**Objectif** : > 80% de couverture

```bash
# AprÃ¨s exÃ©cution des tests
Name                  Stmts   Miss  Cover   Missing
---------------------------------------------------
app/api.py               12      0   100%
app/database.py          45      3    93%   67-69
app/models.py            38      2    95%
app/routes.py            87      5    94%   45, 78, 102
app/schemas.py           25      0   100%
---------------------------------------------------
TOTAL                   207     10    95%
```

### Tests dans le pipeline CI/CD

Les tests s'exÃ©cutent automatiquement sur chaque push/PR :

```yaml
# .github/workflows/ci.yml
jobs:
  tests:
    runs-on: ubuntu-latest
    services:
      db:  # PostgreSQL isolÃ© pour les tests
        image: postgres:16-alpine
        options: --health-cmd pg_isready
    steps:
      - run: uv run pytest
```

---

## DÃ©ploiement

### Pipeline CI/CD automatisÃ©

Le projet utilise **GitHub Actions** pour l'intÃ©gration et le dÃ©ploiement continus :

```yaml
# .github/workflows/ci.yml
name: CI/CD Pipeline

on:
  push:
    branches: [ "master" ]
  pull_request:
    branches: [ "master" ]

jobs:
  tests:
    # 1. ExÃ©cution des tests avec PostgreSQL
    # 2. Validation du code
    # 3. Rapport de couverture

  sync-to-hub:
    needs: tests  # DÃ©ploiement seulement si tests OK
    # 1. Synchronisation vers Hugging Face Spaces
    # 2. DÃ©ploiement automatique
```

### Workflow de dÃ©ploiement

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Git Push    â”‚â”€â”€â”€â”€â”€â–ºâ”‚ Run Tests    â”‚â”€â”€â”€â”€â”€â–ºâ”‚ Deploy to HF â”‚
â”‚  (master)    â”‚      â”‚ (pytest)     â”‚      â”‚ Spaces       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                      â”‚ Tests Failed â”‚
                      â”‚ Stop Pipelineâ”‚
                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### DÃ©ploiement automatique sur Hugging Face Spaces

L'application est dÃ©ployÃ©e automatiquement sur Hugging Face Spaces aprÃ¨s validation des tests unitaires :

**Architecture de production** :
- **Frontend/API** : Hugging Face Spaces (Docker)
- **Base de donnÃ©es** : Supabase (PostgreSQL hÃ©bergÃ©)
- **DÃ©clenchement** : Push sur branche `master` aprÃ¨s succÃ¨s des tests

**Workflow automatisÃ©** :

```yaml
# .github/workflows/ci.yml
jobs:
  tests:
    # 1. ExÃ©cution des tests avec PostgreSQL
    
  sync-to-hub:
    needs: tests  # Ne s'exÃ©cute que si tests passent
    steps:
      - name: Synchroniser vers HF Space
        run: |
          git clone https://$HF_USERNAME:$HF_TOKEN@huggingface.co/spaces/$HF_USERNAME/$HF_SPACE_NAME
          # Copie des fichiers et push automatique
```

**Configuration requise** :

Secrets GitHub nÃ©cessaires :
- `HF_TOKEN` : Token d'authentification Hugging Face
- `HF_USERNAME` : Nom d'utilisateur HF
- `HF_SPACE_NAME` : Nom du Space

Variables d'environnement Hugging Face Space :
```bash
DATABASE_URL=postgresql://user:password@db.supabase.co:5432/postgres
ENVIRONMENT=production
```

**Avantages de Supabase** :
- Base de donnÃ©es PostgreSQL gÃ©rÃ©e
- Backups automatiques
- Scaling automatique
- Interface d'administration
- API REST gÃ©nÃ©rÃ©e automatiquement

---

## SÃ©curitÃ©

### Authentification et contrÃ´le d'accÃ¨s

#### MÃ©thodes implÃ©mentÃ©es

1. **Variables d'environnement sÃ©curisÃ©es**
   ```python
   # app/database.py
   DATABASE_URL = os.getenv("DATABASE_URL", "")
   # Jamais de credentials hardcodÃ©s
   ```

 

2. **Validation stricte des entrÃ©es**
   - Pydantic valide toutes les donnÃ©es
   - PrÃ©vention des injections SQL via SQLAlchemy ORM
   - Typage fort (Python 3.12+)

### Bonnes pratiques de sÃ©curitÃ©

**Gestion des secrets** :
- Variables d'environnement pour les credentials
- Fichier `.env` exclu du contrÃ´le de version (`.gitignore`)
- Secrets GitHub Actions pour le CI/CD
- Secrets Hugging Face pour la production

**SÃ©curitÃ© de la base de donnÃ©es** :
- Connexions PostgreSQL via variables d'environnement
- Pas de credentials hardcodÃ©s dans le code
- Supabase en production (connexions SSL, backups automatiques)
- Connection pooling avec `pool_pre_ping=True`

**Validation des donnÃ©es** :
- Validation stricte via schÃ©mas Pydantic
- Contraintes CHECK dans PostgreSQL
- Typage fort Python 3.12+
- PrÃ©vention des injections SQL via SQLAlchemy ORM


**Notes importantes** :
- L'application actuelle n'implÃ©mente pas d'authentification utilisateur
- Les mots de passe de base de donnÃ©es sont gÃ©rÃ©s via variables d'environnement
- En production sur HF Spaces, la connexion Supabase utilise SSL

---

## Gestion des DonnÃ©es

### Processus de stockage

#### 1. Ingestion des donnÃ©es

```
CSV Source â”€â”€â–º Validation Pydantic â”€â”€â–º Batch Processing â”€â”€â–º PostgreSQL
                      â”‚                      â”‚
                      â–¼                      â–¼
              Error Logging         Transaction Commit
```

**Script d'ingestion** :
```bash
uv run ./app/seed.py \
  --csv-file data_merge.csv \
  --update \
  --batch-size 1000 \
  --database-url postgresql://postgres:mysecretpassword@db:5432/employee_db
```

**Validation automatique** :
- Typage strict des colonnes
- VÃ©rification des contraintes mÃ©tier
- Rejet des doublons

#### 2. Stockage des prÃ©dictions

Chaque appel API â†’ Insertion automatique dans `predictions` :

```sql
INSERT INTO predictions (
    id_employee, prediction, confidence, 
    probability_reste, probability_quitte, model_version
) VALUES (...);
```

### Processus de traitement

#### Pipeline de donnÃ©es

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Employees  â”‚â”€â”€â–ºâ”‚  FastAPI     â”‚â”€â”€â–ºâ”‚ Predictions â”‚
â”‚  (Input)    â”‚   â”‚  Processing  â”‚   â”‚ (Output)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚ XGBoost Modelâ”‚
                  â”‚  Inference   â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Transformation des donnÃ©es

```python
# app/routes.py - prepare_features()
1. Conversion genre (Fâ†’0, Mâ†’1)
2. Normalisation boolÃ©ens
3. Encodage variables catÃ©gorielles
4. CrÃ©ation DataFrame pandas
5. PrÃ©diction XGBoost
```

### Besoins analytiques

#### Tableaux de bord suggÃ©rÃ©s

**1. Dashboard RH - Monitoring du turnover**

```sql
-- Taux de risque par dÃ©partement
SELECT 
    e.departement,
    COUNT(*) as total_predictions,
    SUM(CASE WHEN p.prediction = 1 THEN 1 ELSE 0 END) as at_risk,
    ROUND(AVG(p.confidence), 2) as avg_confidence
FROM predictions p
JOIN employees e ON p.id_employee = e.id_employee
WHERE p.created_at >= NOW() - INTERVAL '30 days'
GROUP BY e.departement
ORDER BY at_risk DESC;
```

**2. Dashboard Analytique - Performance du modÃ¨le**

```sql
-- Ã‰volution de la confiance du modÃ¨le
SELECT 
    DATE(created_at) as date,
    AVG(confidence) as avg_confidence,
    COUNT(*) as predictions_count
FROM predictions
GROUP BY DATE(created_at)
ORDER BY date DESC;
```

**3. Dashboard OpÃ©rationnel - EmployÃ©s Ã  risque**

```sql
-- Liste des employÃ©s high-risk Ã  contacter
SELECT 
    e.id_employee,
    e.departement,
    e.poste,
    e.annees_dans_l_entreprise,
    p.confidence as risk_score,
    p.created_at as derniere_evaluation
FROM employees e
JOIN predictions p ON e.id_employee = p.id_employee
WHERE p.prediction = 1 
  AND p.confidence > 0.75
  AND p.id = (
      SELECT MAX(id) FROM predictions WHERE id_employee = e.id_employee
  )
ORDER BY p.confidence DESC;
```

### Maintenance et archivage

#### StratÃ©gie de rÃ©tention

```sql
-- Archivage des anciennes prÃ©dictions (>1 an)
CREATE TABLE predictions_archive (
    LIKE predictions INCLUDING ALL
);

INSERT INTO predictions_archive 
SELECT * FROM predictions 
WHERE created_at < NOW() - INTERVAL '1 year';

DELETE FROM predictions 
WHERE created_at < NOW() - INTERVAL '1 year';
```

#### Backup automatisÃ©

```bash
# Script de backup journalier
#!/bin/bash
DATE=$(date +%Y%m%d)
pg_dump -h localhost -U postgres employee_db | gzip > backup_$DATE.sql.gz
```

---
